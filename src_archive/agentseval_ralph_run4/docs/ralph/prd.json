{
  "project": "AgentEvals",
  "desciption": "A three-tiered evaluation framework for multi-agent AI systems providing objective benchmarking through traditional metrics, LLM-as-a-Judge assessment, and graph-based complexity analysis",
  "source": "docs/PRD.md",
  "generated": "2026-01-19 00:00:00",
  "stories": [
    {
      "id": "STORY-000",
      "title": "Configuration & Core Data Models",
      "description": "Create JSON config loader and define core Pydantic models (Paper, Review, Evaluation, Metrics, Report) to prevent duplication across evaluation modules",
      "depends_on": [],
      "acceptance": [
        "Create JSON config loader in src/agenteval/config/",
        "Load configuration at application runtime",
        "Define core Pydantic models: Paper, Review, Evaluation, Metrics, Report",
        "Models are reusable across all evaluation modules",
        "Pass all tests in tests/test_config.py and tests/test_models.py"
      ],
      "files": [
        "src/agenteval/config/__init__.py",
        "src/agenteval/config/config.py",
        "src/agenteval/models/__init__.py",
        "src/agenteval/models/data.py",
        "src/agenteval/models/evaluation.py",
        "tests/test_config.py",
        "tests/test_models.py",
        "src/agenteval/config/default.json"
      ],
      "passes": true,
      "completed_at": "2026-01-19T18:45:13Z"
    },
    {
      "id": "STORY-001",
      "title": "Dataset Downloader & Persistence",
      "description": "Download PeerRead dataset from source and persist locally with versioning and checksums for reproducibility and offline use",
      "depends_on": [
        "STORY-000"
      ],
      "acceptance": [
        "Download PeerRead dataset from source",
        "Save dataset locally in structured format",
        "Implement versioning and checksums for integrity verification",
        "Verify dataset completeness after download",
        "Pass all tests in tests/test_downloader.py"
      ],
      "files": [
        "src/agenteval/data/downloader.py",
        "tests/test_downloader.py"
      ],
      "passes": true,
      "completed_at": "2026-01-19T18:49:31Z"
    },
    {
      "id": "STORY-002",
      "title": "Dataset Loader & Parser",
      "description": "Load and parse PeerRead dataset from local storage into structured Pydantic models defined in STORY-000",
      "depends_on": [
        "STORY-000",
        "STORY-001"
      ],
      "acceptance": [
        "Load PeerRead dataset from local storage",
        "Parse into Pydantic models (Paper, Review from Feature 0)",
        "Support batch loading of multiple papers",
        "Return structured data format for downstream processing",
        "Pass all tests in tests/test_peerread.py"
      ],
      "files": [
        "src/agenteval/data/peerread.py",
        "tests/test_peerread.py"
      ],
      "passes": true,
      "completed_at": "2026-01-19T18:53:49Z"
    },
    {
      "id": "STORY-003",
      "title": "Traditional Metrics Calculator",
      "description": "Calculate execution time, task success rate, and coordination quality metrics with structured JSON output",
      "depends_on": [
        "STORY-000",
        "STORY-002"
      ],
      "acceptance": [
        "Calculate execution time metrics for agent task completion",
        "Measure task success rate across evaluation runs",
        "Assess coordination quality between agents",
        "Output metrics in structured JSON format",
        "Support batch evaluation of multiple agent outputs"
      ],
      "files": [
        "src/agenteval/metrics/traditional.py",
        "tests/test_traditional.py"
      ],
      "passes": true,
      "completed_at": "2026-01-19T18:57:55Z"
    },
    {
      "id": "STORY-004",
      "title": "LLM Judge Evaluator",
      "description": "Evaluate semantic quality of agent-generated reviews using PydanticAI LLM judge against human baseline reviews from PeerRead",
      "depends_on": [
        "STORY-000",
        "STORY-002"
      ],
      "acceptance": [
        "Evaluate semantic quality of provided agent-generated reviews",
        "Compare agent outputs against human baseline reviews from PeerRead",
        "Provide scoring with justification from LLM judge",
        "Support configurable evaluation criteria",
        "Use mock/sample agent outputs for testing",
        "Pass all tests in tests/test_llm_judge.py"
      ],
      "files": [
        "src/agenteval/judges/__init__.py",
        "src/agenteval/judges/llm_judge.py",
        "tests/test_llm_judge.py"
      ],
      "passes": true,
      "completed_at": "2026-01-19T21:51:15Z"
    },
    {
      "id": "STORY-005",
      "title": "Graph Complexity Analyzer",
      "description": "Model agent interactions as NetworkX graphs and calculate complexity metrics (density, centrality, clustering coefficient)",
      "depends_on": [
        "STORY-000"
      ],
      "acceptance": [
        "Model agent interactions as graph structures using NetworkX",
        "Calculate complexity metrics (density, centrality, clustering coefficient) from interaction graphs",
        "Identify coordination patterns between agents",
        "Export graph data in JSON/GraphML format for external analysis",
        "Use mock/sample interaction data for testing",
        "Pass all tests in tests/test_graph.py"
      ],
      "files": [
        "src/agenteval/metrics/graph.py",
        "tests/test_graph.py"
      ],
      "passes": true,
      "completed_at": "2026-01-19T21:55:44Z"
    },
    {
      "id": "STORY-006",
      "title": "Evaluation Pipeline Orchestrator",
      "description": "Orchestrate execution of all three evaluation tiers (traditional, LLM judge, graph) in sequence with dependency management and reproducibility controls",
      "depends_on": [
        "STORY-003",
        "STORY-004",
        "STORY-005"
      ],
      "acceptance": [
        "Run all three evaluation tiers (traditional, LLM judge, graph) in sequence",
        "Handle module dependencies correctly",
        "Support reproducible runs with seed configuration",
        "Collect results from all modules",
        "Pass results to reporting module",
        "Pass all tests in tests/test_pipeline.py"
      ],
      "files": [
        "src/agenteval/pipeline.py",
        "tests/test_pipeline.py"
      ],
      "passes": true,
      "completed_at": "2026-01-19T22:00:59Z"
    },
    {
      "id": "STORY-007",
      "title": "Consolidated Reporting & Observability",
      "description": "Combine results from all evaluation tiers into unified JSON report with loguru console logging and optional Logfire/Weave cloud export",
      "depends_on": [
        "STORY-006"
      ],
      "acceptance": [
        "Combine results from all three evaluation tiers",
        "Generate consolidated JSON report with all metrics",
        "Integrate loguru for local console tracing by default",
        "Support optional Logfire/Weave cloud export via configuration",
        "Output combined results in structured format",
        "Pass all tests in tests/test_report.py"
      ],
      "files": [
        "src/agenteval/report.py",
        "tests/test_report.py"
      ],
      "passes": true,
      "completed_at": "2026-01-19T22:07:16Z"
    }
  ]
}
