{
  "project": {
    "name": "AgentEvals",
    "description": "A three-tiered evaluation framework for multi-agent AI systems that provides objective benchmarking of autonomous agent teams. Uses the PeerRead dataset to generate and evaluate scientific paper reviews through traditional metrics, LLM-as-a-Judge assessment, and graph-based complexity analysis.",
    "goals": [
      "Provide standardized, reproducible evaluation of multi-agent system outputs",
      "Enable objective comparison of different agent implementations",
      "Deliver multi-tiered insights combining performance, semantic, and structural metrics"
    ],
    "target_users": "AI Researchers and ML Engineers working with multi-agent systems"
  },
  "stories": [
    {
      "id": "STORY-001",
      "title": "PeerRead Dataset Loader",
      "description": "Implement loader for PeerRead dataset to support scientific review evaluation",
      "feature": "Feature 2: LLM-as-a-Judge Evaluation",
      "user_story": "As a researcher, I want to load and parse PeerRead dataset papers and reviews so that I can evaluate agent-generated reviews against human baselines",
      "acceptance": [
        "Load and parse PeerRead dataset papers and reviews",
        "Support structured data format for papers and reviews",
        "Handle dataset errors and missing data gracefully",
        "Provide iterator interface for batch processing"
      ],
      "technical_requirements": [
        "PeerRead dataset integration for scientific reviews",
        "Async HTTP client (httpx>=0.28) for dataset downloads",
        "Pydantic models for data validation"
      ],
      "dependencies": {
        "httpx": ">=0.28",
        "pydantic": ">=2.10.6"
      },
      "files": [
        "src/agenteval/data/peerread.py",
        "tests/test_peerread.py"
      ],
      "priority": "high",
      "estimated_complexity": "medium"
    },
    {
      "id": "STORY-002",
      "title": "Traditional Performance Metrics",
      "description": "Implement traditional performance metrics for agent system evaluation",
      "feature": "Feature 1: Traditional Performance Metrics",
      "user_story": "As an ML engineer, I want to measure agent system performance with standard metrics so that I can compare different implementations objectively",
      "acceptance": [
        "Calculate execution time metrics for agent task completion",
        "Measure task success rate across evaluation runs",
        "Assess coordination quality between agents",
        "Output metrics in structured JSON format",
        "Support batch evaluation of multiple agent outputs"
      ],
      "technical_requirements": [
        "PydanticAI for agent implementation",
        "JSON output format for metrics",
        "Deterministic results with seed configuration",
        "String similarity metrics using rapidfuzz>=3.0 (Levenshtein, Jaro-Winkler)",
        "ML metrics using scikit-learn>=1.7 (F1, precision, recall, cosine similarity)"
      ],
      "dependencies": {
        "rapidfuzz": ">=3.0",
        "scikit-learn": ">=1.7",
        "pydantic": ">=2.10.6"
      },
      "files": [
        "src/agenteval/metrics/traditional.py",
        "tests/test_traditional.py"
      ],
      "priority": "high",
      "estimated_complexity": "medium"
    },
    {
      "id": "STORY-003",
      "title": "LLM-as-a-Judge Evaluation",
      "description": "Implement LLM-based semantic quality assessment for agent-generated reviews",
      "feature": "Feature 2: LLM-as-a-Judge Evaluation",
      "user_story": "As a researcher, I want LLM-based quality assessment so that I can evaluate semantic review quality beyond traditional metrics",
      "acceptance": [
        "Evaluate semantic quality of agent-generated reviews",
        "Compare agent outputs against human baseline reviews",
        "Provide scoring with justification from LLM judge",
        "Support configurable evaluation criteria",
        "Handle API errors and rate limits gracefully"
      ],
      "technical_requirements": [
        "PydanticAI (pydantic-ai-slim>=0.8.1) for LLM judge agent",
        "Structured evaluation output with scores and reasoning",
        "OpenAI plugin for LLM calls",
        "Pydantic models for structured outputs"
      ],
      "dependencies": {
        "pydantic-ai-slim": ">=0.8.1",
        "pydantic": ">=2.10.6"
      },
      "files": [
        "src/agenteval/judges/llm_judge.py",
        "tests/test_llm_judge.py"
      ],
      "priority": "high",
      "estimated_complexity": "medium"
    },
    {
      "id": "STORY-004",
      "title": "Graph-Based Complexity Analysis",
      "description": "Implement graph-based structural analysis of agent interaction patterns",
      "feature": "Feature 3: Graph-Based Complexity Analysis",
      "user_story": "As a data scientist, I want graph-based structural analysis so that I can understand agent interaction patterns and coordination complexity",
      "acceptance": [
        "Model agent interactions as graph structures using NetworkX",
        "Calculate complexity metrics (density, centrality, clustering coefficient) from interaction graphs",
        "Identify coordination patterns between agents",
        "Export graph data for external analysis"
      ],
      "technical_requirements": [
        "Use NetworkX (networkx>=3.6) for graph representation of agent interactions",
        "NetworkX-based complexity metrics (graph density, degree centrality, betweenness centrality, clustering coefficient)",
        "JSON/GraphML export format"
      ],
      "dependencies": {
        "networkx": ">=3.6"
      },
      "files": [
        "src/agenteval/metrics/graph.py",
        "tests/test_graph.py"
      ],
      "priority": "high",
      "estimated_complexity": "medium"
    },
    {
      "id": "STORY-005",
      "title": "Unified Evaluation Pipeline",
      "description": "Combine all three evaluation tiers into a unified pipeline with consolidated reporting and observability",
      "feature": "Feature 4: Unified Evaluation Pipeline",
      "user_story": "As a researcher, I want a unified evaluation pipeline so that I can run all three evaluation approaches together",
      "acceptance": [
        "Run all three evaluation tiers in sequence",
        "Generate consolidated evaluation report",
        "Support reproducible runs with seed configuration",
        "Output combined results in structured format",
        "Provide local console tracing by default",
        "Support optional Logfire/Weave cloud export via configuration"
      ],
      "technical_requirements": [
        "Pipeline orchestration for all three tiers",
        "Consolidated JSON report format",
        "Seed-based reproducibility",
        "Local console tracing by default using loguru>=0.7",
        "Optional Logfire (logfire>=3.16) cloud export via config",
        "Optional Weave (weave>=0.51) W&B integration via config"
      ],
      "dependencies": {
        "loguru": ">=0.7",
        "logfire": ">=3.16",
        "weave": ">=0.51",
        "pydantic-settings": ">=2.9.1"
      },
      "files": [
        "src/agenteval/pipeline.py",
        "src/agenteval/report.py",
        "tests/test_pipeline.py"
      ],
      "priority": "high",
      "estimated_complexity": "high"
    }
  ],
  "non_functional_requirements": {
    "performance": [
      "Evaluation runs efficiently on standard research computing resources",
      "Batch processing of multiple papers without memory issues"
    ],
    "reproducibility": [
      "Consistent evaluation results across multiple runs with same seed",
      "Version-locked dependencies via uv.lock"
    ],
    "code_quality": [
      "All code must pass `make validate` (ruff, pyright, pytest)",
      "Follow KISS, DRY, YAGNI principles",
      "Python 3.13+ with modern typing features",
      "Test-driven development with pytest"
    ]
  },
  "out_of_scope": [
    "Production deployment infrastructure or scaling",
    "Real-time streaming evaluation of agent outputs",
    "Support for agent frameworks beyond PydanticAI",
    "Multi-language support for non-English reviews"
  ]
}
